layer00:
  0.txt_attn.proj: torch.float8_e4m3fn
  0.txt_attn.qkv: torch.float8_e4m3fn
  0.txt_mlp.0: torch.float8_e4m3fn
  0.txt_mlp.2: torch.float8_e4m3fn
  0.txt_mod.lin: torch.float8_e4m3fn
  loss: 0.00028375693364068866
  sensitivity: 0.14428271194373815
  time: 2870.75
layer01:
  1.txt_attn.proj: torch.float8_e4m3fn
  1.txt_attn.qkv: torch.float8_e4m3fn
  1.txt_mlp.0: torch.float8_e4m3fn
  1.txt_mlp.2: torch.float8_e4m3fn
  1.txt_mod.lin: torch.float8_e4m3fn
  loss: 0.0014962863642722368
  sensitivity: 0.17911877077913854
  time: 19.906000000075437
layer02:
  2.txt_attn.proj: torch.float8_e4m3fn
  2.txt_attn.qkv: torch.float8_e4m3fn
  2.txt_mlp.0: torch.float8_e4m3fn
  2.txt_mlp.2: torch.float8_e4m3fn
  2.txt_mod.lin: torch.float8_e4m3fn
  loss: 0.0035771450493484735
  sensitivity: 0.24404543881125834
  time: 19.125
layer03:
  3.txt_attn.proj: torch.float8_e4m3fn
  3.txt_attn.qkv: torch.float8_e4m3fn
  3.txt_mlp.0: torch.float8_e4m3fn
  3.txt_mlp.2: torch.float8_e4m3fn
  3.txt_mod.lin: torch.float8_e4m3fn
  loss: 0.0018050683429464698
  sensitivity: 0.16453458958135853
  time: 20.0
layer04:
  4.txt_attn.proj: torch.float8_e4m3fn
  4.txt_attn.qkv: torch.float8_e4m3fn
  4.txt_mlp.0: torch.float8_e4m3fn
  4.txt_mlp.2: torch.float8_e4m3fn
  4.txt_mod.lin: torch.float8_e4m3fn
  loss: 0.0015010300558060408
  sensitivity: 0.14510619638051203
  time: 20.140999999945052
layer05:
  5.txt_attn.proj: torch.float8_e4m3fn
  5.txt_attn.qkv: torch.float8_e4m3fn
  5.txt_mlp.0: torch.float8_e4m3fn
  5.txt_mlp.2: torch.float8_e4m3fn
  5.txt_mod.lin: torch.float8_e4m3fn
  loss: 0.0010674690129235387
  sensitivity: 0.1084694956127442
  time: 19.56299999996554
layer06:
  6.txt_attn.proj: torch.float8_e4m3fn
  6.txt_attn.qkv: torch.float8_e4m3fn
  6.txt_mlp.0: torch.float8_e4m3fn
  6.txt_mlp.2: torch.float8_e4m3fn
  6.txt_mod.lin: torch.float8_e4m3fn
  loss: 0.001051638857461512
  sensitivity: 0.10569617066872952
  time: 2911.969000000041
layer07:
  7.txt_attn.proj: torch.float8_e4m3fn
  7.txt_attn.qkv: torch.float8_e4m3fn
  7.txt_mlp.0: torch.float8_e4m3fn
  7.txt_mlp.2: torch.float8_e4m3fn
  7.txt_mod.lin: torch.float8_e4m3fn
  loss: 0.0013375368434935808
  sensitivity: 0.11379574657511353
  time: 21.531000000075437
layer08:
  8.txt_attn.proj: torch.float8_e4m3fn
  8.txt_attn.qkv: torch.float8_e4m3fn
  8.txt_mlp.0: torch.float8_e4m3fn
  8.txt_mlp.2: torch.float8_e4m3fn
  8.txt_mod.lin: torch.float8_e4m3fn
  loss: 0.0012369254836812615
  sensitivity: 0.10455238634040923
  time: 20.766000000061467
layer09:
  9.txt_attn.proj: torch.float8_e4m3fn
  9.txt_attn.qkv: torch.float8_e4m3fn
  9.txt_mlp.0: torch.float8_e4m3fn
  9.txt_mlp.2: torch.float8_e4m3fn
  9.txt_mod.lin: torch.float8_e4m3fn
  loss: 0.0014431205345317721
  sensitivity: 0.10536630004500255
  time: 20.5
layer10:
  10.txt_attn.proj: torch.float8_e4m3fn
  10.txt_attn.qkv: torch.float8_e4m3fn
  10.txt_mlp.0: torch.float8_e4m3fn
  10.txt_mlp.2: torch.float8_e4m3fn
  10.txt_mod.lin: torch.float8_e4m3fn
  loss: 0.00167128536850214
  sensitivity: 0.082999452443382
  time: 20.780999999959022
layer11:
  11.txt_attn.proj: torch.float8_e4m3fn
  11.txt_attn.qkv: torch.float8_e4m3fn
  11.txt_mlp.0: torch.float8_e4m3fn
  11.txt_mlp.2: torch.float8_e4m3fn
  11.txt_mod.lin: torch.float8_e4m3fn
  loss: 0.0014970885822549462
  sensitivity: 0.07402803242948904
  time: 21.344000000040978
layer12:
  12.txt_attn.proj: torch.float8_e4m3fn
  12.txt_attn.qkv: torch.float8_e4m3fn
  12.txt_mlp.0: torch.float8_e4m3fn
  12.txt_mlp.2: torch.float8_e4m3fn
  12.txt_mod.lin: torch.float8_e4m3fn
  loss: 0.0018328179139643908
  sensitivity: 0.06428934335529848
  time: 21.51500000001397
layer13:
  13.txt_attn.proj: torch.float8_e4m3fn
  13.txt_attn.qkv: torch.float8_e4m3fn
  13.txt_mlp.0: torch.float8_e4m3fn
  13.txt_mlp.2: torch.float8_e4m3fn
  13.txt_mod.lin: torch.float8_e4m3fn
  loss: 0.0018866395112127066
  sensitivity: 0.05951597760555829
  time: 1760.7029999999795
layer14:
  14.txt_attn.proj: torch.float8_e4m3fn
  14.txt_attn.qkv: torch.float8_e4m3fn
  14.txt_mlp.0: torch.float8_e4m3fn
  14.txt_mlp.2: torch.float8_e4m3fn
  14.txt_mod.lin: torch.float8_e4m3fn
  loss: 0.002972183283418417
  sensitivity: 0.04756604032887899
  time: 22.75
layer15:
  15.txt_attn.proj: torch.float8_e4m3fn
  15.txt_attn.qkv: torch.float8_e4m3fn
  15.txt_mlp.0: torch.float8_e4m3fn
  15.txt_mlp.2: torch.float8_e4m3fn
  15.txt_mod.lin: torch.float8_e4m3fn
  loss: 0.00264522316865623
  sensitivity: 0.04173456197048837
  time: 22.17099999997299
layer16:
  16.txt_attn.proj: torch.float8_e4m3fn
  16.txt_attn.qkv: torch.float8_e4m3fn
  16.txt_mlp.0: torch.float8_e4m3fn
  16.txt_mlp.2: torch.float8_e4m3fn
  16.txt_mod.lin: torch.float8_e4m3fn
  loss: 0.0030851829797029495
  sensitivity: 0.039596373692872576
  time: 22.21799999999348
layer17:
  17.txt_attn.proj: torch.float8_e4m3fn
  17.txt_attn.qkv: torch.float8_e4m3fn
  17.txt_mlp.0: torch.float8_e4m3fn
  17.txt_mlp.2: torch.float8_e4m3fn
  17.txt_mod.lin: torch.float8_e4m3fn
  loss: 0.005839254707098007
  sensitivity: 0.046249558996489866
  time: 22.21799999999348
layer18:
  18.txt_attn.proj: torch.float8_e4m3fn
  18.txt_attn.qkv: torch.float8_e4m3fn
  18.txt_mlp.0: torch.float8_e4m3fn
  18.txt_mlp.2: torch.float8_e4m3fn
  18.txt_mod.lin: torch.float8_e4m3fn
  loss: .inf
  sensitivity: .inf
  time: 275.98400000005495

